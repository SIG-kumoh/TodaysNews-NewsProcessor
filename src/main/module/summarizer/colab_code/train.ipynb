{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pip install lightning==2.0.8\n",
    "%pip install loguru\n",
    "%pip install transformers==4.32.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightning as L\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class KoBARTSummaryDataset(Dataset):\n",
    "    def __init__(self, file, tokenizer, max_len, ignore_index=-100):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.docs = pd.read_csv(file, sep='\\t')\n",
    "        self.len = self.docs.shape[0]\n",
    "\n",
    "        self.pad_index = self.tokenizer.pad_token_id\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def add_padding_data(self, inputs):\n",
    "        if len(inputs) < self.max_len:\n",
    "            pad = np.array([self.pad_index] * (self.max_len - len(inputs)))\n",
    "            inputs = np.concatenate([inputs, pad])\n",
    "        else:\n",
    "            inputs = inputs[:self.max_len]\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def add_ignored_data(self, inputs):\n",
    "        if len(inputs) < self.max_len:\n",
    "            pad = np.array([self.ignore_index] * (self.max_len - len(inputs)))\n",
    "            inputs = np.concatenate([inputs, pad])\n",
    "        else:\n",
    "            inputs = inputs[:self.max_len]\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        instance = self.docs.iloc[idx]\n",
    "        input_ids = self.tokenizer.encode(instance['news'])\n",
    "        input_ids = self.add_padding_data(input_ids)\n",
    "\n",
    "        label_ids = self.tokenizer.encode(instance['summary'])\n",
    "        label_ids.append(self.tokenizer.eos_token_id)\n",
    "        dec_input_ids = [self.tokenizer.eos_token_id]\n",
    "        dec_input_ids += label_ids[:-1]\n",
    "        dec_input_ids = self.add_padding_data(dec_input_ids)\n",
    "        label_ids = self.add_ignored_data(label_ids)\n",
    "\n",
    "        return {'input_ids': np.array(input_ids, dtype=np.int_),\n",
    "                'decoder_input_ids': np.array(dec_input_ids, dtype=np.int_),\n",
    "                'labels': np.array(label_ids, dtype=np.int_)\n",
    "                }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "class KobartSummaryModule(L.LightningDataModule):\n",
    "    def __init__(self, train_file,\n",
    "                 test_file, tok,\n",
    "                 max_len=512,\n",
    "                 batch_size=8,\n",
    "                 num_workers=4):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.max_len = max_len\n",
    "        self.train_file_path = train_file\n",
    "        self.test_file_path = test_file\n",
    "        self.tok = tok\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = argparse.ArgumentParser(\n",
    "            parents=[parent_parser], add_help=False)\n",
    "        parser.add_argument('--num_workers',\n",
    "                            type=int,\n",
    "                            default=4,\n",
    "                            help='num of worker for dataloader')\n",
    "        return parser\n",
    "\n",
    "    # OPTIONAL, called for every GPU/machine (assigning state is OK)\n",
    "    def setup(self, stage):\n",
    "        # split dataset\n",
    "        self.train = KoBARTSummaryDataset(self.train_file_path,\n",
    "                                          self.tok,\n",
    "                                          self.max_len)\n",
    "        self.test = KoBARTSummaryDataset(self.test_file_path,\n",
    "                                         self.tok,\n",
    "                                         self.max_len)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train = DataLoader(self.train,\n",
    "                           batch_size=self.batch_size,\n",
    "                           num_workers=self.num_workers, shuffle=True)\n",
    "        return train\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val = DataLoader(self.test,\n",
    "                         batch_size=self.batch_size,\n",
    "                         num_workers=self.num_workers, shuffle=False)\n",
    "        return val\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test = DataLoader(self.test,\n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers, shuffle=False)\n",
    "        return test\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7c30f9ee92a4103"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import lightning as L\n",
    "\n",
    "from collections import defaultdict\n",
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
    "from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "class KoBARTConditionalGeneration(L.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            hparams,\n",
    "            **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        self.model = BartForConditionalGeneration.from_pretrained('digit82/kobart-summarization')\n",
    "        self.model.train()\n",
    "        self.bos_token = '<s>'\n",
    "        self.eos_token = '</s>'\n",
    "\n",
    "        self.tokenizer = PreTrainedTokenizerFast.from_pretrained('digit82/kobart-summarization')\n",
    "        self.pad_token_id = self.tokenizer.pad_token_id\n",
    "\n",
    "        self.outputs = defaultdict(list)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Prepare optimizer\n",
    "        param_optimizer = list(self.model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(\n",
    "                nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(\n",
    "                nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                          lr=self.hparams.lr, correct_bias=False)\n",
    "        num_workers = self.hparams.num_workers\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=int(self.trainer.estimated_stepping_batches * 0.1),\n",
    "            num_training_steps=self.trainer.estimated_stepping_batches,\n",
    "        )\n",
    "\n",
    "        lr_scheduler = {'scheduler': scheduler,\n",
    "                        'monitor': 'loss', 'interval': 'step',\n",
    "                        'frequency': 1}\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        attention_mask = inputs['input_ids'].ne(self.pad_token_id).float()\n",
    "        decoder_attention_mask = inputs['decoder_input_ids'].ne(self.pad_token_id).float()\n",
    "\n",
    "        return self.model(input_ids=inputs['input_ids'],\n",
    "                          attention_mask=attention_mask,\n",
    "                          decoder_input_ids=inputs['decoder_input_ids'],\n",
    "                          decoder_attention_mask=decoder_attention_mask,\n",
    "                          labels=inputs['labels'].type(torch.LongTensor), return_dict=True)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outs = self(batch)\n",
    "        loss = outs.loss\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        outs = self(batch)\n",
    "        loss = outs['loss']\n",
    "        self.outputs[dataloader_idx].append({\"loss\": loss})\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        flat_outputs = []\n",
    "        for lst in self.outputs.values():\n",
    "            flat_outputs.extend(lst)\n",
    "        loss = torch.stack([x[\"loss\"] for x in flat_outputs]).mean()\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.outputs.clear()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c0140a195a58bae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import argparse\n",
    "import lightning as L\n",
    "\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from loguru import logger\n",
    "from transformers import PreTrainedTokenizerFast"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd1c9888d69bed96"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2625d81a2499148c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='KoBART Summarization')\n",
    "\n",
    "\n",
    "class ArgsBase:\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = argparse.ArgumentParser(\n",
    "            parents=[parent_parser], add_help=False)\n",
    "        parser.add_argument('--train_file',\n",
    "                            type=str,\n",
    "                            default='/content/drive/MyDrive/data/train.tsv',\n",
    "                            help='train file')\n",
    "        parser.add_argument('--test_file',\n",
    "                            type=str,\n",
    "                            default='/content/drive/MyDrive/data/test.tsv',\n",
    "                            help='test file')\n",
    "        parser.add_argument('--batch_size',\n",
    "                            type=int,\n",
    "                            default=8,\n",
    "                            help='')\n",
    "        parser.add_argument('--checkpoint',\n",
    "                            type=str,\n",
    "                            default='/content/drive/MyDrive/data/checkpoint',\n",
    "                            help='')\n",
    "        parser.add_argument('--max_len',\n",
    "                            type=int,\n",
    "                            default=512,\n",
    "                            help='max seq len')\n",
    "        parser.add_argument('--max_epochs',\n",
    "                            type=int,\n",
    "                            default=10,\n",
    "                            help='train epochs')\n",
    "        parser.add_argument('--lr',\n",
    "                            type=float,\n",
    "                            default=3e-5,\n",
    "                            help='The initial learning rate')\n",
    "        parser.add_argument('--accelerator',\n",
    "                            type=str,\n",
    "                            default='gpu',\n",
    "                            choices=['gpu', 'cpu'],\n",
    "                            help='select accelerator')\n",
    "        parser.add_argument('--num_gpus',\n",
    "                            type=int,\n",
    "                            default=1,\n",
    "                            help='number of gpus')\n",
    "        parser.add_argument('--gradient_clip_val',\n",
    "                            type=float,\n",
    "                            default=1.0,\n",
    "                            help='gradient_clipping')\n",
    "\n",
    "        return parser"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33324560e54dd97c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parser = ArgsBase.add_model_specific_args(parser)\n",
    "parser = KobartSummaryModule.add_model_specific_args(parser)\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('digit82/kobart-summarization')\n",
    "args = parser.parse_args('')\n",
    "logger.info(args)\n",
    "\n",
    "dm = KobartSummaryModule(args.train_file,\n",
    "                          args.test_file,\n",
    "                          tokenizer,\n",
    "                          batch_size=args.batch_size,\n",
    "                          max_len=args.max_len,\n",
    "                          num_workers=args.num_workers)\n",
    "\n",
    "dm.setup('fit')\n",
    "\n",
    "model = KoBARTConditionalGeneration(args)\n",
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss',\n",
    "                                      dirpath=args.checkpoint,\n",
    "                                      filename='model_chp/{epoch:02d}-{val_loss:.3f}',\n",
    "                                      verbose=True,\n",
    "                                      save_last=True,\n",
    "                                      mode='min',\n",
    "                                      save_top_k=3)\n",
    "\n",
    "trainer = L.Trainer(max_epochs=args.max_epochs,\n",
    "                    accelerator=args.accelerator,\n",
    "                    devices=args.num_gpus,\n",
    "                    gradient_clip_val=args.gradient_clip_val,\n",
    "                    callbacks=[checkpoint_callback]\n",
    "                    )\n",
    "\n",
    "trainer.fit(model, dm)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6631cfee4bcedfce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import argparse\n",
    "from transformers.models.bart import BartForConditionalGeneration\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--hparams\", default='/content/drive/MyDrive/data/checkpoint/hparams.yaml', type=str)\n",
    "parser.add_argument(\"--model_binary\", default='/content/drive/MyDrive/data/checkpoint/model_chp/epoch=01-val_loss=1.627.ckpt', type=str)\n",
    "parser.add_argument(\"--output_dir\", default='/content/drive/MyDrive/data/kobart_summary', type=str)\n",
    "args = parser.parse_args('')\n",
    "\n",
    "inf = KoBARTConditionalGeneration.load_from_checkpoint(args.model_binary)\n",
    "\n",
    "inf.model.save_pretrained(args.output_dir)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce0e29b0e895831d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
